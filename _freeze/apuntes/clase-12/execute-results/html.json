{
  "hash": "f56da0fbd656df3f0fc5058d9b116faa",
  "result": {
    "engine": "knitr",
    "markdown": "# Regularización {-}\n\n\n::: {.callout-important}\n- Tanto Ridge como Lasso **suponen normalidad de los residuos**.\n- Para los métodos de Ridge y Lasso no contamos con tests para saber\nqué tan significativos son las covariables en el modelo. \n\n- No contamos con una forma de la **distribución de los estimadores** $\\beta$.\nEsto se complica debido a que el estimador es **sesgado**.\n:::\n\n## Ejercicios\n\n- Calcular la varianza de Ridge.\n\n\n## Introducción\n\n- Si se permite un pequeño sesgo en el estimador de $\\beta$,\nes posible reducir su varianza, lo que puede dar lugar a un ECM menor que \nel del estimador insesgado $\\hat{\\beta}$ de MCO.\n\n- Esto se traduce en estimadores más estables y \nen intervalos de confianza más angostos.\n\n\n\n## Regresion Ridge\n\n### Observaciones\n\n- Se **mantienen todos los predictores**, pero se **contraen hacia cero** sus coeficientes,\nsin que lleguen exactamente a cero. Esto reduce la varianza de las estimaciones.\n\n- Se **penalizan los coeficientes grandes**, forzándolos a ser más pequeños que los\nestimados por mínimos cuadrados.\n\n- El objetivo es mejorar la generalización del modelo.\n\n::: {.callout-important}\n- Eficiente cuando tienes que emplear todas las covariables.\n- Eficiente cuando existe alta dimensionalidad.\n- Eficiente cuando existe multicolinealidad entres los datos.\n:::\n\n\n::: {.callout-note}\n### Definición\n\nEl **estimador de Ridge** es la solución de \n\n$$\n\\hat{\\beta}^{\\text{ridge}} = \\arg\\min_{\\beta} \\left\\{ (y - X\\beta)^{\\top}(y - X\\beta) + \\lambda \\beta^{\\top} \\beta \\right\\}\n$$\n\ndonde $\\lambda \\geq 0$ es conocido como el parámetro de regularización o penalización.\n:::\n\n\n::: {.callout-caution}\nLa penalización $\\lambda$ reducirá todos los coeficientes hacia cero,\npero no establecerá ninguno de ellos exactamente en cero (salvo $\\lambda =\\infty$).\n\nEsto puede no ser un problema para la precisión de la predicción, \npero **dificulta la interpretación del modelo para p grande**.\n:::\n\nPara un $\\lambda$ fijo, deseamos que los $\\beta$ sean más pequeños (su penalización),\npara tener menor sesgo y varianza del modelo.\n\n## Regresion LASSO\n\n- Estima algunos parámetros a **exactamente cero**.\n\n::: {.callout-important}\n- Eficiente para seleccionar qué covariables emplear para el modelo.\n- Eficiente para tener un modelo menos complejo, **más interpretable**.\n:::\n\n\n::: {.callout-note}\n### Definición\n\nSea el modelo lineal $y_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta} + \\varepsilon_i$.\nEl estimador de Lasso (Least Absolute Shrinkage and Selection Operator) se define como la solución de\n\n$$\n\\hat{\\boldsymbol{\\beta}}^{\\text{lasso}} = \\arg\\min_{\\boldsymbol{\\beta}} \\left\\{ \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta})^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\right\\}\n$$\n\ndonde $\\lambda$ es un parámetro de precisión. Si $\\lambda = 0$, regresión lineal tradicional $(\\hat{\\boldsymbol{\\beta}}^{\\text{lasso}} = \\hat{\\boldsymbol{\\beta}})$.\n\n:::\n\n\n::: {.callout-note}\n### No visto en clase\n\n| Value of $p$ | Regularization Behavior                                      | When it's useful                                                                                               |\n| ------------ | ------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------- |\n| $p = 2$      | Smooth shrinkage of all coefficients                         | When multicollinearity is present and you want all variables retained with small coefficients.                 |\n| $p = 1$      | Sparsity (some coefficients set to zero)                     | When feature selection is important, or many features are irrelevant.                                          |\n| $p < 1$      | **Stronger** sparsity than Lasso                             | When only a few features are truly relevant, but you're willing to trade off convexity.                        |\n| $p > 2$      | **Heavier penalty** on large coefficients, but less sparsity | May be useful when you want to heavily penalize outliers in coefficients but not induce sparsity. Rarely used. |\n\n:::\n\n\n## Ejemplos prácticos\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ISLR)\nlibrary(ggplot2)\n\ndata(\"Hitters\")\nnames(Hitters)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"AtBat\"     \"Hits\"      \"HmRun\"     \"Runs\"      \"RBI\"       \"Walks\"    \n [7] \"Years\"     \"CAtBat\"    \"CHits\"     \"CHmRun\"    \"CRuns\"     \"CRBI\"     \n[13] \"CWalks\"    \"League\"    \"Division\"  \"PutOuts\"   \"Assists\"   \"Errors\"   \n[19] \"Salary\"    \"NewLeague\"\n```\n\n\n:::\n\n```{.r .cell-code}\nstr(Hitters)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t322 obs. of  20 variables:\n $ AtBat    : int  293 315 479 496 321 594 185 298 323 401 ...\n $ Hits     : int  66 81 130 141 87 169 37 73 81 92 ...\n $ HmRun    : int  1 7 18 20 10 4 1 0 6 17 ...\n $ Runs     : int  30 24 66 65 39 74 23 24 26 49 ...\n $ RBI      : int  29 38 72 78 42 51 8 24 32 66 ...\n $ Walks    : int  14 39 76 37 30 35 21 7 8 65 ...\n $ Years    : int  1 14 3 11 2 11 2 3 2 13 ...\n $ CAtBat   : int  293 3449 1624 5628 396 4408 214 509 341 5206 ...\n $ CHits    : int  66 835 457 1575 101 1133 42 108 86 1332 ...\n $ CHmRun   : int  1 69 63 225 12 19 1 0 6 253 ...\n $ CRuns    : int  30 321 224 828 48 501 30 41 32 784 ...\n $ CRBI     : int  29 414 266 838 46 336 9 37 34 890 ...\n $ CWalks   : int  14 375 263 354 33 194 24 12 8 866 ...\n $ League   : Factor w/ 2 levels \"A\",\"N\": 1 2 1 2 2 1 2 1 2 1 ...\n $ Division : Factor w/ 2 levels \"E\",\"W\": 1 2 2 1 1 2 1 2 2 1 ...\n $ PutOuts  : int  446 632 880 200 805 282 76 121 143 0 ...\n $ Assists  : int  33 43 82 11 40 421 127 283 290 0 ...\n $ Errors   : int  20 10 14 3 4 25 7 9 19 0 ...\n $ Salary   : num  NA 475 480 500 91.5 750 70 100 75 1100 ...\n $ NewLeague: Factor w/ 2 levels \"A\",\"N\": 1 2 1 2 2 1 1 1 2 1 ...\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(Hitters)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun\n-Andy Allanson      293   66     1   30  29    14     1    293    66      1\n-Alan Ashby         315   81     7   24  38    39    14   3449   835     69\n-Alvin Davis        479  130    18   66  72    76     3   1624   457     63\n-Andre Dawson       496  141    20   65  78    37    11   5628  1575    225\n-Andres Galarraga   321   87    10   39  42    30     2    396   101     12\n-Alfredo Griffin    594  169     4   74  51    35    11   4408  1133     19\n                  CRuns CRBI CWalks League Division PutOuts Assists Errors\n-Andy Allanson       30   29     14      A        E     446      33     20\n-Alan Ashby         321  414    375      N        W     632      43     10\n-Alvin Davis        224  266    263      A        W     880      82     14\n-Andre Dawson       828  838    354      N        E     200      11      3\n-Andres Galarraga    48   46     33      N        E     805      40      4\n-Alfredo Griffin    501  336    194      A        W     282     421     25\n                  Salary NewLeague\n-Andy Allanson        NA         A\n-Alan Ashby        475.0         N\n-Alvin Davis       480.0         A\n-Andre Dawson      500.0         N\n-Andres Galarraga   91.5         N\n-Alfredo Griffin   750.0         A\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(Hitters)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 322  20\n```\n\n\n:::\n\n```{.r .cell-code}\n# Eliminando datos perdidos\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n\n\n:::\n\n```{.r .cell-code}\nHitters = Hitters %>%\n  na.omit\n\nx=model.matrix(Salary~.,Hitters)[,-1] # eliminar la primera columna\n# dejando solo los predictores\ny=Hitters$Salary\n```\n:::\n\n\n\n\n\n### Regresión Ridge\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Matrix\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoaded glmnet 4.1-9\n```\n\n\n:::\n\n```{.r .cell-code}\n# Busqueda de valores para lambda (desde 10^10 hasta 10^-2 )\ngrid=10^seq(10,-2,length=100)\ngrid\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [1] 1.000000e+10 7.564633e+09 5.722368e+09 4.328761e+09 3.274549e+09\n  [6] 2.477076e+09 1.873817e+09 1.417474e+09 1.072267e+09 8.111308e+08\n [11] 6.135907e+08 4.641589e+08 3.511192e+08 2.656088e+08 2.009233e+08\n [16] 1.519911e+08 1.149757e+08 8.697490e+07 6.579332e+07 4.977024e+07\n [21] 3.764936e+07 2.848036e+07 2.154435e+07 1.629751e+07 1.232847e+07\n [26] 9.326033e+06 7.054802e+06 5.336699e+06 4.037017e+06 3.053856e+06\n [31] 2.310130e+06 1.747528e+06 1.321941e+06 1.000000e+06 7.564633e+05\n [36] 5.722368e+05 4.328761e+05 3.274549e+05 2.477076e+05 1.873817e+05\n [41] 1.417474e+05 1.072267e+05 8.111308e+04 6.135907e+04 4.641589e+04\n [46] 3.511192e+04 2.656088e+04 2.009233e+04 1.519911e+04 1.149757e+04\n [51] 8.697490e+03 6.579332e+03 4.977024e+03 3.764936e+03 2.848036e+03\n [56] 2.154435e+03 1.629751e+03 1.232847e+03 9.326033e+02 7.054802e+02\n [61] 5.336699e+02 4.037017e+02 3.053856e+02 2.310130e+02 1.747528e+02\n [66] 1.321941e+02 1.000000e+02 7.564633e+01 5.722368e+01 4.328761e+01\n [71] 3.274549e+01 2.477076e+01 1.873817e+01 1.417474e+01 1.072267e+01\n [76] 8.111308e+00 6.135907e+00 4.641589e+00 3.511192e+00 2.656088e+00\n [81] 2.009233e+00 1.519911e+00 1.149757e+00 8.697490e-01 6.579332e-01\n [86] 4.977024e-01 3.764936e-01 2.848036e-01 2.154435e-01 1.629751e-01\n [91] 1.232847e-01 9.326033e-02 7.054802e-02 5.336699e-02 4.037017e-02\n [96] 3.053856e-02 2.310130e-02 1.747528e-02 1.321941e-02 1.000000e-02\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Ajustar la regresion ridge\n# alpha=0 indica regresión ridge; los predictores son estandarizados por defecto.\nridge.mod=glmnet(x,y,alpha=0,lambda=grid) # por defecto standardize = TRUE\nridge.mod\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  glmnet(x = x, y = y, alpha = 0, lambda = grid) \n\n    Df  %Dev    Lambda\n1   19  0.00 1.000e+10\n2   19  0.00 7.565e+09\n3   19  0.00 5.722e+09\n4   19  0.00 4.329e+09\n5   19  0.00 3.275e+09\n6   19  0.00 2.477e+09\n7   19  0.00 1.874e+09\n8   19  0.00 1.417e+09\n9   19  0.00 1.072e+09\n10  19  0.00 8.111e+08\n11  19  0.00 6.136e+08\n12  19  0.00 4.642e+08\n13  19  0.00 3.511e+08\n14  19  0.00 2.656e+08\n15  19  0.00 2.009e+08\n16  19  0.00 1.520e+08\n17  19  0.00 1.150e+08\n18  19  0.00 8.697e+07\n19  19  0.00 6.579e+07\n20  19  0.01 4.977e+07\n21  19  0.01 3.765e+07\n22  19  0.01 2.848e+07\n23  19  0.01 2.154e+07\n24  19  0.02 1.630e+07\n25  19  0.02 1.233e+07\n26  19  0.03 9.326e+06\n27  19  0.04 7.055e+06\n28  19  0.05 5.337e+06\n29  19  0.07 4.037e+06\n30  19  0.09 3.054e+06\n31  19  0.12 2.310e+06\n32  19  0.16 1.748e+06\n33  19  0.21 1.322e+06\n34  19  0.27 1.000e+06\n35  19  0.36 7.565e+05\n36  19  0.48 5.722e+05\n37  19  0.63 4.329e+05\n38  19  0.83 3.275e+05\n39  19  1.09 2.477e+05\n40  19  1.43 1.874e+05\n41  19  1.88 1.417e+05\n42  19  2.46 1.072e+05\n43  19  3.21 8.111e+04\n44  19  4.16 6.136e+04\n45  19  5.38 4.642e+04\n46  19  6.90 3.511e+04\n47  19  8.77 2.656e+04\n48  19 11.03 2.009e+04\n49  19 13.70 1.520e+04\n50  19 16.75 1.150e+04\n51  19 20.11 8.697e+03\n52  19 23.69 6.579e+03\n53  19 27.31 4.977e+03\n54  19 30.81 3.765e+03\n55  19 34.04 2.848e+03\n56  19 36.88 2.154e+03\n57  19 39.28 1.630e+03\n58  19 41.25 1.233e+03\n59  19 42.84 9.330e+02\n60  19 44.12 7.060e+02\n61  19 45.16 5.340e+02\n62  19 46.01 4.040e+02\n63  19 46.73 3.050e+02\n64  19 47.37 2.310e+02\n65  19 47.95 1.750e+02\n66  19 48.50 1.320e+02\n67  19 49.04 1.000e+02\n68  19 49.57 7.600e+01\n69  19 50.11 5.700e+01\n70  19 50.66 4.300e+01\n71  19 51.19 3.300e+01\n72  19 51.71 2.500e+01\n73  19 52.19 1.900e+01\n74  19 52.63 1.400e+01\n75  19 53.03 1.100e+01\n76  19 53.36 8.000e+00\n77  19 53.65 6.000e+00\n78  19 53.88 5.000e+00\n79  19 54.07 4.000e+00\n80  19 54.21 3.000e+00\n81  19 54.32 2.000e+00\n82  19 54.41 2.000e+00\n83  19 54.47 1.000e+00\n84  19 54.51 1.000e+00\n85  19 54.55 1.000e+00\n86  19 54.57 0.000e+00\n87  19 54.58 0.000e+00\n88  19 54.59 0.000e+00\n89  19 54.60 0.000e+00\n90  19 54.60 0.000e+00\n91  19 54.60 0.000e+00\n92  19 54.60 0.000e+00\n93  19 54.61 0.000e+00\n94  19 54.61 0.000e+00\n95  19 54.61 0.000e+00\n96  19 54.61 0.000e+00\n97  19 54.61 0.000e+00\n98  19 54.61 0.000e+00\n99  19 54.61 0.000e+00\n100 19 54.61 0.000e+00\n```\n\n\n:::\n\n```{.r .cell-code}\n# Dimension de la matriz que almacena los coeficientes de la regresion ridge\ndim(coef(ridge.mod)) #Una fila por cada predictor y una colunma por valor de lambda\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  20 100\n```\n\n\n:::\n\n```{.r .cell-code}\n# Grafica de los coeficientes \nplot(ridge.mod,  xvar = \"lambda\", label = TRUE)      \n```\n\n::: {.cell-output-display}\n![](clase-12_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n\n- `%Dev`: \n  - Porcentaje de explicación de esas covariables en el modelo\n  - Similar a un $R^2$.\n  - Buscamos un **balance** entre eso valor y $\\lambda$.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Se espera que los coeficientes se contraigan (tiendan a cero)\n# a medida que lambda se incrementa.\nridge.mod$lambda[50] # Mostrar el valor 50 de lambda\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 11497.57\n```\n\n\n:::\n\n```{.r .cell-code}\ncoef(ridge.mod)[,50] # Mostrar los coeficientes asociados con el valor 50 de lambda\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept)         AtBat          Hits         HmRun          Runs \n407.356050200   0.036957182   0.138180344   0.524629976   0.230701523 \n          RBI         Walks         Years        CAtBat         CHits \n  0.239841459   0.289618741   1.107702929   0.003131815   0.011653637 \n       CHmRun         CRuns          CRBI        CWalks       LeagueN \n  0.087545670   0.023379882   0.024138320   0.025015421   0.085028114 \n    DivisionW       PutOuts       Assists        Errors    NewLeagueN \n -6.215440973   0.016482577   0.002612988  -0.020502690   0.301433531 \n```\n\n\n:::\n\n```{.r .cell-code}\nsqrt(sum(coef(ridge.mod)[-1,50]^2)) # Calcular la norma L2 (suma de cuadrados)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.360612\n```\n\n\n:::\n\n```{.r .cell-code}\n# Comparando con el valor 60\nridge.mod$lambda[60]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 705.4802\n```\n\n\n:::\n\n```{.r .cell-code}\ncoef(ridge.mod)[,60]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n (Intercept)        AtBat         Hits        HmRun         Runs          RBI \n 54.32519950   0.11211115   0.65622409   1.17980910   0.93769713   0.84718546 \n       Walks        Years       CAtBat        CHits       CHmRun        CRuns \n  1.31987948   2.59640425   0.01083413   0.04674557   0.33777318   0.09355528 \n        CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists \n  0.09780402   0.07189612  13.68370191 -54.65877750   0.11852289   0.01606037 \n      Errors   NewLeagueN \n -0.70358655   8.61181213 \n```\n\n\n:::\n\n```{.r .cell-code}\nsqrt(sum(coef(ridge.mod)[-1,60]^2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 57.11001\n```\n\n\n:::\n\n```{.r .cell-code}\n# Los coeficientes para lambda = 50.\npredict(ridge.mod,s=50,type=\"coefficients\")[1:20,]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept)         AtBat          Hits         HmRun          Runs \n 4.876610e+01 -3.580999e-01  1.969359e+00 -1.278248e+00  1.145892e+00 \n          RBI         Walks         Years        CAtBat         CHits \n 8.038292e-01  2.716186e+00 -6.218319e+00  5.447837e-03  1.064895e-01 \n       CHmRun         CRuns          CRBI        CWalks       LeagueN \n 6.244860e-01  2.214985e-01  2.186914e-01 -1.500245e-01  4.592589e+01 \n    DivisionW       PutOuts       Assists        Errors    NewLeagueN \n-1.182011e+02  2.502322e-01  1.215665e-01 -3.278600e+00 -9.496680e+00 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Dividiendo el conjunto de datos para estimar el error de test\nset.seed(1)\ntrain=sample(1:nrow(x), nrow(x)/2)\ntest=(-train)\ny.test=y[test]\n\n# Estimamos la regresion ridge en el conjunto de entrenamiento \nridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)\n\n# Evaluamos el MSE en el conjunto de test para un lambda = 4\nridge.pred=predict(ridge.mod,s=4,newx=x[test,])\nmean((ridge.pred-y.test)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 142199.2\n```\n\n\n:::\n\n```{.r .cell-code}\n# Comparando con el MSE del modelo nulo (media)\nmean((mean(y[train])-y.test)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 224669.9\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Esto se aproxima a la predicción del modelo nulo cuando lambda tiende infinito.\nridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])\nmean((ridge.pred-y.test)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 224669.8\n```\n\n\n:::\n\n```{.r .cell-code}\n# Comparando con MCO que equivale a ridge con lambda = 0.\nridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train])\nmean((ridge.pred-y.test)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 168588.6\n```\n\n\n:::\n\n```{.r .cell-code}\nlm(y~x, subset=train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x, subset = train)\n\nCoefficients:\n(Intercept)       xAtBat        xHits       xHmRun        xRuns         xRBI  \n   274.0145      -0.3521      -1.6377       5.8145       1.5424       1.1243  \n     xWalks       xYears      xCAtBat       xCHits      xCHmRun       xCRuns  \n     3.7287     -16.3773      -0.6412       3.1632       3.4008      -0.9739  \n      xCRBI      xCWalks     xLeagueN   xDivisionW     xPutOuts     xAssists  \n    -0.6005       0.3379     119.1486    -144.0831       0.1976       0.6804  \n    xErrors  xNewLeagueN  \n    -4.7128     -71.0951  \n```\n\n\n:::\n\n```{.r .cell-code}\npredict(ridge.mod,s=0,exact=T,type=\"coefficients\",x=x[train,],y=y[train])[1:20,]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n (Intercept)        AtBat         Hits        HmRun         Runs          RBI \n 274.0200994   -0.3521900   -1.6371383    5.8146692    1.5423361    1.1241837 \n       Walks        Years       CAtBat        CHits       CHmRun        CRuns \n   3.7288406  -16.3795195   -0.6411235    3.1629444    3.4005281   -0.9739405 \n        CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists \n  -0.6003976    0.3378422  119.1434637 -144.0853061    0.1976300    0.6804200 \n      Errors   NewLeagueN \n  -4.7127879  -71.0898914 \n```\n\n\n:::\n:::\n\n\n\n\n\n#### Validación cruzada\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Realizando la validacion cruzada para seleccionar el valor de lambda\nset.seed(1)\n\n# Ajustar la regresion ridge a los datos de entrenamiento\ncv.out=cv.glmnet(x[train,],y[train],alpha=0) \nbestlam=cv.out$lambda.min # elegir el valor de lambda que minimiza el MSE\nbestlam\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 326.0828\n```\n\n\n:::\n\n```{.r .cell-code}\n# Graficando el MSE por cv en funcion de lambda\nplot(cv.out)\n```\n\n::: {.cell-output-display}\n![](clase-12_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\nridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])\nmean((ridge.pred-y.test)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 139856.6\n```\n\n\n:::\n\n```{.r .cell-code}\n# Modelo final (con todos los datos)\nout=glmnet(x,y,alpha=0)\npredict(out,type=\"coefficients\",s=bestlam)[1:20,]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n (Intercept)        AtBat         Hits        HmRun         Runs          RBI \n 15.44383120   0.07715547   0.85911582   0.60103106   1.06369007   0.87936105 \n       Walks        Years       CAtBat        CHits       CHmRun        CRuns \n  1.62444617   1.35254778   0.01134999   0.05746654   0.40680157   0.11456224 \n        CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists \n  0.12116504   0.05299202  22.09143197 -79.04032656   0.16619903   0.02941950 \n      Errors   NewLeagueN \n -1.36092945   9.12487765 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Nota: Para elegir el valor de lambda segun la regla de 1 error estandar\n# lambda.1se corresponde al valor de lambda más grande cuyo error\n# de validación está dentro de 1 error estándar del mínimo.\ncv.out$lambda.1se \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6401.138\n```\n\n\n:::\n:::\n\n\n\n\n\n### Regresión Lasso\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Ajustar el modelo a los datos de entrenamiento\nlasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)\nplot(lasso.mod, xvar = \"lambda\", label = TRUE) \n```\n\n::: {.cell-output-display}\n![](clase-12_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Realizando la validacion cruzada para seleccionar el valor de lambda\nset.seed(1)\n\n# Ajustar el modelo a los datos de entrenamiento \ncv.out=cv.glmnet(x[train,],y[train],alpha=1)\nplot(cv.out)\n```\n\n::: {.cell-output-display}\n![](clase-12_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Seleccionar el valor de lambda que minimiza el MSE\nbestlam=cv.out$lambda.min\nbestlam\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 9.286955\n```\n\n\n:::\n\n```{.r .cell-code}\n# Usar el valor de lambda para predecir los datos de test\nlasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])\n\n# MSE de test\nmean((lasso.pred-y.test)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 143673.6\n```\n\n\n:::\n\n```{.r .cell-code}\n# Estimacion del modelo final (todos los datos)\nout=glmnet(x,y,alpha=1,lambda=grid)\nlasso.coef=predict(out,type=\"coefficients\",s=bestlam)[1:20,]\nlasso.coef\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept)         AtBat          Hits         HmRun          Runs \n   1.27479059   -0.05497143    2.18034583    0.00000000    0.00000000 \n          RBI         Walks         Years        CAtBat         CHits \n   0.00000000    2.29192406   -0.33806109    0.00000000    0.00000000 \n       CHmRun         CRuns          CRBI        CWalks       LeagueN \n   0.02825013    0.21628385    0.41712537    0.00000000   20.28615023 \n    DivisionW       PutOuts       Assists        Errors    NewLeagueN \n-116.16755870    0.23752385    0.00000000   -0.85629148    0.00000000 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Coeficientes distintos de 0\nlasso.coef[lasso.coef!=0]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept)         AtBat          Hits         Walks         Years \n   1.27479059   -0.05497143    2.18034583    2.29192406   -0.33806109 \n       CHmRun         CRuns          CRBI       LeagueN     DivisionW \n   0.02825013    0.21628385    0.41712537   20.28615023 -116.16755870 \n      PutOuts        Errors \n   0.23752385   -0.85629148 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Regresión robusta\n\n### Introducción\n\n- El método de MCO no es tan eficiente en caso **existan valores atípicos** influyentes.\n\n- En la práctica, la distribución de la variable de respuesta puede no ser normal\ny presentar valores atípicos que afectan el modelo de regresión\n\n- Cuando la distribución tiene colas largas, se generan observaciones extremas\nque influyen en exceso sobre los estimadores por mínimos cuadrados, sesgando\nla ecuación de regresión.\n\n- La regresión robusta surge como una alternativa para enfrentar estas limitaciones, ofreciendo estimadores menos sensibles a valores atípicos y estructuras de\nerror no normales.\n\n::: {.callout-note}\n### Definición\n\nLa regresión robusta es una técnica de estimación que busca **limitar la influencia de valores atípicos** en un modelo de regresión, produciendo estimadores más estables cuando los datos no cumplen los supuestos ideales de los mínimos cuadrados.\n:::\n\nTipos de valores atípicos:\n\n- **Valor atípico de regresión**: \nEs un punto que se desvía de la regresión lineal que se determina con las $n-1$\nobservaciones restantes.\n\n- **Valor atípico residual**: \nEs un punto que tiene un residual estandarizado o studentizado grande, cuando se\nusa en la muestra de observaciones con que se ajusta un modelo.\n\n- **Valor atípico en el espacio X**:\nEs una observación remota en una o más coordenadas $x$.\n\n- **Valor atípico en el espacio Y**:\nEs una observación con coordenada $y$ inusual. El efecto que tiene la observación sobre el modelo de regresión depende de su coordenada $x$ y de la disposición general de las demás observaciones en la muestra.\n\n- **Valores atípicos en los espacios X y Y**:\nEs una observación que es atípica en sus coordenadas $x$ e $y$ al mismo tiempo. El efecto de esos puntos depende por completo de la disposición de las demás observaciones de la muestra.\n\n::: {.callout-important}\n\n- Este modelo considera una distribución diferente para los residuos, \nuna con **colas más pesadas**.\n\n- Considere como ejemplo minimizar la suma de los **valores absolutos** de los errores.\n\n- Recuerde: EMV aplicado al modelo con errores de normal, conduce al MCO.\n\n- Nótese que el criterio del **error absoluto ponderaría los valores atípicos con mucho menos severidad que los MCO**.\n\n:::\n\n::: {.callout-note}\n### Definición\n\nLos estimadores de $\\beta$ se obtienen *minimizando* una función particular en $\\beta$:\n\n$$\\arg \\min_{\\beta} \\sum_{i=1}^{n} \\rho(\\epsilon_i) = \\arg \\min_{\\beta} \\sum_{i=1}^{n} \\rho(y_i - \\mathbf{x}_i^{\\top} \\beta)$$\n\ndonde $\\rho$ es una función de la *contribución* de cada residuo a la función objetivo. Un $\\rho$ razonable debe cumplir las siguientes propiedades:\n\n- Siempre no negativo: $\\rho(\\epsilon) \\geq 0$.\n\n- Igual a cero cuando su argumento es cero: $\\rho(0) = 0$.\n\n- Simétrico: $\\rho(\\epsilon) = \\rho(-\\epsilon)$, aunque en algunos problemas la simetría es indeseable.\n\n- Monótono en $|\\epsilon_i|$: $\\rho(\\epsilon_i) \\geq \\rho(\\epsilon_i')$ para $|\\epsilon_i| > |\\epsilon_i'|$.\n\n:::",
    "supporting": [
      "clase-12_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}